{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I still should check that boltzmann stuff is working properly before doing the Q-values, so try running the boltzmann on the reward matrix rather than the q values and check that it gives you the distribution you'd expect just to make sure I'm not messing anything up there. \n",
    "\n",
    "Then if that's all fine, which it probably is, have another look at the Q-value and V-Value calculations in learn, and if they look right then step through and check that it's doing what I expect them to be doing. \n",
    "\n",
    "If it's something wrong with one of those then that might fix the learning side of things as well, but at least we'll have a proper trajectory to learn from because atm the one it's creating, and the resulting policy is abysmal, so fix that first, get the expert to perform more expertly and then you can check to see if there are problems with the learning algorithm :) \n",
    "\n",
    "Issue has to be in policy iteration somewhere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from env import SingleStateSpace \n",
    "from generate_trajectory import generate_trajectory\n",
    "from policy import Boltzmann \n",
    "from learn import compute_q_with_values\n",
    "from RewardFunctions import SingleStateReward\n",
    "import numpy as np \n",
    "from learn import policy_iteration\n",
    "from utils import normalise_pi\n",
    "from policy_walk import policy_walk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trajectory_length = 20 \n",
    "#set up the agent and their policy \n",
    "env = SingleStateSpace(n_actions = 10, discount_rate = 0.1, R_max = 5)\n",
    "#so we have access to env.reward\n",
    "observation_times = np.cumsum(np.random.uniform(0, 2, size=trajectory_length))\n",
    "print(\"Generated observation times:\") \n",
    "print(observation_times)\n",
    "\n",
    "#this isn't right but for now I'm just normalising pi - really it should be a boltzmann - later on I turn the Q-vals into a distribution via boltzmann \n",
    "#so it won't be able to learn i don't think \n",
    "\n",
    "\n",
    "# Use meshgrid to create a grid of all possible combinations of s, a, and t\n",
    "s_grid, a_grid, t_grid = np.meshgrid(env.states, env.actions, observation_times, indexing='ij')\n",
    "R = np.vectorize(env.reward)(s_grid,a_grid,t_grid)\n",
    "\n",
    "# Calculate the reward for each combination using vectorized operations - oh I'm not actually using a boltzmann policy here I'm just using a normalised reward vector? \n",
    "(optimal_pi, optimal_values) = policy_iteration(env, len(observation_times), R)\n",
    "optimal_q_values = np.ones((env.n_states, env.n_actions, len(observation_times)))\n",
    "for s in range(env.n_states):\n",
    "    for a in range(env.n_actions): \n",
    "        for t in range(len(observation_times)):\n",
    "            optimal_q_values[s,a,t] = compute_q_with_values(env,s,a,t,optimal_values,R) \n",
    "\n",
    "boltzmann_policy = Boltzmann(optimal_q_values, env.actions)\n",
    "\n",
    "#What is my initial policy? Everywhere I've seen, boltzmann is defined using the Q-values but you\n",
    "#need a policy to find the Q-values so what is the actual policy? \n",
    "#Is it just like an exponential style distribution over the rewards? \n",
    "observations = generate_trajectory(env, boltzmann_policy, observation_times)\n",
    "\n",
    "print(\"Generated trajectory\") \n",
    "print(observations)\n",
    "\n",
    "print(\"Running policy walk\") \n",
    "learned_pi = policy_walk(env, observations, optimal_q_values) # there's a better way than passing in the Q* values but I cba dealing with it now \n",
    "print(\"finished learning policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.ones((env.n_states,len(observations)))\n",
    "import matplotlib.pyplot as plt \n",
    "for i in range(5):\n",
    "    for s in range(env.n_states):\n",
    "        for t in range(len(observations)): \n",
    "            pi[s,t] = boltzmann_policy(s,t)\n",
    "    plt.plot(pi[0])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observation_times) \n",
    "for t in observation_times: # I should check I'm actually using the observation_times in the reward I bet I'm not and that'd have me shifted off by 2 each time which could cause the weird cycling rewards that I'm seeing \n",
    "    #nah it's not that, I create R properly and if I was using env.reward anywhere other than when I create R I'd be getting errors. \n",
    "    stuff = [] \n",
    "    for a_ in range(env.n_actions): \n",
    "        stuff.append(env.reward(0,a_,t))\n",
    "    plt.plot(stuff)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb6416ccb7cf3862052581a5888563209785f6bca63738835247a0de3829cc25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
