{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I still should check that boltzmann stuff is working properly before doing the Q-values, so try running the boltzmann on the reward matrix rather than the q values and check that it gives you the distribution you'd expect just to make sure I'm not messing anything up there. \n",
    "\n",
    "Then if that's all fine, which it probably is, have another look at the Q-value and V-Value calculations in learn, and if they look right then step through and check that it's doing what I expect them to be doing. \n",
    "\n",
    "If it's something wrong with one of those then that might fix the learning side of things as well, but at least we'll have a proper trajectory to learn from because atm the one it's creating, and the resulting policy is abysmal, so fix that first, get the expert to perform more expertly and then you can check to see if there are problems with the learning algorithm :) \n",
    "\n",
    "Issue has to be in policy iteration somewhere "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "from env import SingleStateSpace \n",
    "from generate_trajectory import generate_trajectory\n",
    "from policy import Boltzmann \n",
    "from learn import compute_q_with_values\n",
    "from RewardFunctions import SingleStateReward\n",
    "import numpy as np \n",
    "from learn import policy_iteration\n",
    "from utils import normalise_pi\n",
    "from policy_walk import policy_walk \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated observation times:\n",
      "[ 0.70071022  1.94935825  2.60494554  3.24491636  3.24983847  3.87262539\n",
      "  3.98477175  5.65226309  6.70209691  8.29156639 10.10643484 11.79140937\n",
      " 11.80764542 12.65219259 12.91667317 14.08660098 15.13130241 15.4880177\n",
      " 16.17398077 16.2282645 ]\n",
      "Generated trajectory\n",
      "[[0, 1, 0], [0, 5, 1], [0, 1, 2], [0, 7, 3], [0, 3, 4], [0, 2, 5], [0, 4, 6], [0, 5, 7], [0, 3, 8], [0, 8, 9], [0, 9, 10], [0, 7, 11], [0, 8, 12], [0, 2, 13], [0, 9, 14], [0, 1, 15], [0, 5, 16], [0, 4, 17], [0, 3, 18], [0, 4, 19]]\n",
      ".\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \n\u001b[0;32m     31\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     learned_rewards\u001b[39m.\u001b[39mappend((policy_walk(env,observations,optimal_q_values)))\n\u001b[0;32m     34\u001b[0m \u001b[39m# print(\"Running policy walk\") \u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# learned_R = policy_walk(env, observations, optimal_q_values) # there's a better way than passing in the Q* values but I cba dealing with it now \u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m# print(\"finished learning policy\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomms\\Desktop\\IRLProject\\src\\policy_walk.py:19\u001b[0m, in \u001b[0;36mpolicy_walk\u001b[1;34m(env, observations, optimal_q_values, step_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mwhile\u001b[39;00m iters \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m: \n\u001b[0;32m     18\u001b[0m     R_tild \u001b[39m=\u001b[39m get_neighbouring_reward(R, step_size) \n\u001b[1;32m---> 19\u001b[0m     (pi_tild, values_tild) \u001b[39m=\u001b[39m learn\u001b[39m.\u001b[39;49mpolicy_iteration(env, n_observations, R_tild, pi \u001b[39m=\u001b[39;49m pi)\n\u001b[0;32m     21\u001b[0m     q_tild \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((env\u001b[39m.\u001b[39mn_states, env\u001b[39m.\u001b[39mn_actions, n_observations))\n\u001b[0;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39mn_states):\n",
      "File \u001b[1;32mc:\\Users\\tomms\\Desktop\\IRLProject\\src\\learn.py:38\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[1;34m(env, n_observations, R, delta, pi)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39mn_actions): \n\u001b[0;32m     37\u001b[0m     q_vals[a] \u001b[39m=\u001b[39m compute_q_with_values(env,s,a,t,values,R)\n\u001b[1;32m---> 38\u001b[0m pi[s,t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax(q_vals) \n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m b \u001b[39m!=\u001b[39m pi[s,t]:\n\u001b[0;32m     40\u001b[0m     policy_stable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tomms\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\tomms\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "trajectory_length = 20 \n",
    "#set up the agent and their policy \n",
    "env = SingleStateSpace(n_actions = 10, discount_rate = 0.1, R_max = 5)\n",
    "#so we have access to env.reward\n",
    "observation_times = np.cumsum(np.random.uniform(0, 2, size=trajectory_length))\n",
    "print(\"Generated observation times:\") \n",
    "print(observation_times)\n",
    "\n",
    "# Use meshgrid to create a grid of all possible combinations of s, a, and t\n",
    "s_grid, a_grid, t_grid = np.meshgrid(env.states, env.actions, observation_times, indexing='ij')\n",
    "R = np.vectorize(env.reward)(s_grid,a_grid,t_grid)\n",
    "\n",
    "# Calculate the reward for each combination using vectorized operations - oh I'm not actually using a boltzmann policy here I'm just using a normalised reward vector? \n",
    "(optimal_pi, optimal_values) = policy_iteration(env, len(observation_times), R)\n",
    "optimal_q_values = np.ones((env.n_states, env.n_actions, len(observation_times)))\n",
    "for s in range(env.n_states):\n",
    "    for a in range(env.n_actions): \n",
    "        for t in range(len(observation_times)):\n",
    "            optimal_q_values[s,a,t] = compute_q_with_values(env,s,a,t,optimal_values,R) \n",
    "\n",
    "b = Boltzmann(optimal_q_values, env.actions, alpha = 1.5)\n",
    "observations = generate_trajectory(env,b,observation_times)\n",
    "\n",
    "print(\"Generated trajectory\") \n",
    "print(observations)\n",
    "\n",
    "n_samples = 100 \n",
    "learned_rewards = [] \n",
    "for i in range(n_samples): \n",
    "    if i%10 == 0: \n",
    "        print(\".\")\n",
    "    learned_rewards.append((policy_walk(env,observations,optimal_q_values)))\n",
    "\n",
    "# print(\"Running policy walk\") \n",
    "# learned_R = policy_walk(env, observations, optimal_q_values) # there's a better way than passing in the Q* values but I cba dealing with it now \n",
    "# print(\"finished learning policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(learned_R.shape)\n",
    "for t in range(len(observations)):\n",
    "    plt.plot(learned_R[0,:,t])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = Boltzmann(optimal_q_values, env.actions)\n",
    "\n",
    "#What is my initial policy? Everywhere I've seen, boltzmann is defined using the Q-values but you\n",
    "#need a policy to find the Q-values so what is the actual policy? \n",
    "#Is it just like an exponential style distribution over the rewards? \n",
    "# observations = generate_trajectory(env, boltzmann_policy, observation_times)\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2, 4, 8, 16, 32, 64]\n",
    "for alpha in alphas: \n",
    "    b = Boltzmann(optimal_q_values,env.actions,alpha) \n",
    "    \n",
    "    count = np.zeros(env.n_actions)\n",
    "    for i in range(100): \n",
    "        count[b(0,0)] += 1\n",
    "    plt.plot(count) \n",
    "    asd = \"alpha = \" + format(alpha)\n",
    "    plt.title(asd)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "for t in range(len(observations)):\n",
    "    plt.plot(R[0,:,t])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb6416ccb7cf3862052581a5888563209785f6bca63738835247a0de3829cc25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
