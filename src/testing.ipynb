{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from env import SingleStateSpace \n",
    "from env import DoubleStateSpace\n",
    "from RewardFunctions import SingleStateReward\n",
    "from RewardFunctions import DoubleStateReward\n",
    "\n",
    "from policy_walk import PolicyWalk \n",
    "from priors import UniformPrior \n",
    "from priors import TimeDependentPrior\n",
    "from priors import SkewSymmetricGaussianPrior\n",
    "\n",
    "from generate_trajectory import generate_trajectory\n",
    "from policy import Boltzmann \n",
    "from learn import policy_iteration\n",
    "\n",
    "import tensorflow_probability as tfp \n",
    "\n",
    "import os \n",
    "import cProfile\n",
    "import re \n",
    "# cProfile.run('re.compile(pw.get_samples(step_size, n_iters = 1000))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the environment\n",
    "# trajectory_length = 10\n",
    "# env = SingleStateSpace(n_actions = 10, discount_rate = 0.1, R_max = 5)\n",
    "\n",
    "# #generate observation times \n",
    "# # observation_times = np.cumsum(np.random.uniform(0, 2, size=trajectory_length))\n",
    "# observation_times = [1.85,2.54,4.13,5.09,7.02,7.89,9.86,11.82,12.03,12.87]\n",
    "\n",
    "# # Create a matrix for the true rewards \n",
    "# s_grid, a_grid, t_grid = np.meshgrid(env.states, env.actions, observation_times, indexing='ij')\n",
    "# R = np.vectorize(env.reward)(s_grid,a_grid,t_grid)\n",
    "\n",
    "# #Find the optimal policy, values and q_values for the true reward to generate the trajectory. \n",
    "# alpha = 2\n",
    "# (optimal_pi, optimal_values, optimal_q_values) = policy_iteration(env,trajectory_length,R)\n",
    "# b = Boltzmann(optimal_q_values, env.actions, alpha = alpha)\n",
    "\n",
    "# #multiple trajectories are combined - just means that the agent does multiple things at each time slot to allow for more data to be gathered\n",
    "# observations = generate_trajectory(env,b,observation_times, n_trajectories=50) \n",
    "\n",
    "# # mcmc_steps = [\"grid_walk\", \"gaussian_scalar\"]\n",
    "# # priors = [\"ssgp\", \"tdp single\", \"tdp full\"]\n",
    "# mcmc_steps = [\"gaussian_scalar\"]\n",
    "# priors = [\"ssgp\", \"tdp single\"]\n",
    "# for mcmc_step in mcmc_steps: \n",
    "#     for p in priors: \n",
    "#         # if mcmc_step == \"gaussian_scalar\" and p != \"tdp full\": \n",
    "#         #     continue \n",
    "        \n",
    "#         # else:\n",
    "#         aps = [] \n",
    "#         lrs = [] \n",
    "#         # sigmas = [0.5,1,1.5]\n",
    "#         # step_sizes = [0.01, 0.05, 0.1]\n",
    "#         sigmas = [0.01,0.1,0.5,1,1.5,2]\n",
    "#         step_sizes = [0.005,0.01,0.05,0.1,0.5,1]\n",
    "#         if p == \"ssgp\": \n",
    "#             sigmas = [None] \n",
    "#         for step_size in step_sizes: \n",
    "#             for sigma in sigmas: \n",
    "#                 if p == \"ssgp\": \n",
    "#                     prior = SkewSymmetricGaussianPrior()\n",
    "#                 elif p == \"tdp single\": \n",
    "#                     prior = TimeDependentPrior(observation_times, R_max = 5, sigma = sigma, full = False) \n",
    "#                 elif p == \"tdp full\": \n",
    "#                     prior = TimeDependentPrior(observation_times, R_max = 5, sigma = sigma, full = True)\n",
    "\n",
    "#                 pw = PolicyWalk(env,prior,observations,observation_times,alpha,mcmc_step = mcmc_step)\n",
    "#                 learned_rewards, acceptance_probs = pw.get_samples(step_size, n_iters = 100000)\n",
    "#                 aps.append(acceptance_probs) \n",
    "#                 lrs.append(learned_rewards)\n",
    "#         name = p + \" \" + mcmc_step\n",
    "#         folderloc, AP_folder_loc, LR_folder_loc, chain_folder_loc = save(True, name)\n",
    "#         doAcceptanceAnalysis(AP_folder_loc)\n",
    "#         doChainAnalysis(chain_folder_loc)\n",
    "#         doLearnedRewardAnalysis(0,2, LR_folder_loc)\n",
    "#         doLossAnalysis(folderloc)\n",
    "\n",
    "trajectory_length = 10\n",
    "# env = DoubleStateSpace(n_actions = 10, discount_rate = 0.1, R_max = 5)\n",
    "\n",
    "# #generate observation times \n",
    "# # observation_times = np.cumsum(np.random.uniform(0, 2, size=trajectory_length))\n",
    "# observation_times = np.load(\"Double State Experiments/observation_times.npy\")\n",
    "\n",
    "# # Create a matrix for the true rewards \n",
    "# s_grid, a_grid, t_grid = np.meshgrid(env.states, env.actions, observation_times, indexing='ij')\n",
    "# R = np.vectorize(env.reward)(s_grid,a_grid,t_grid)\n",
    "\n",
    "# #Find the optimal policy, values and q_values for the true reward to generate the trajectory. \n",
    "# alpha = 2\n",
    "# (optimal_pi, optimal_values, optimal_q_values) = policy_iteration(env,trajectory_length,R)\n",
    "# b = Boltzmann(optimal_q_values, env.actions, alpha = alpha)\n",
    "\n",
    "# #multiple trajectories are combined - just means that the agent does multiple things at each time slot to allow for more data to be gathered\n",
    "# # observations = generate_trajectory(env,b,observation_times, n_trajectories=50) \n",
    "\n",
    "# observations = np.load(\"Double State Experiments/observations.npy\")\n",
    "\n",
    "\n",
    "# mcmc_steps = [\"gaussian_scalar\"]\n",
    "# priors = [\"ssgp\", \"tdp single\"]\n",
    "mcmc_steps = [\"gaussian_scalar\",\"grid_walk\"]\n",
    "priors = [\"ssgp\", \"tdp single\", \"tdp full\"]\n",
    "for mcmc_step in mcmc_steps: \n",
    "    for p in priors: \n",
    "        # if mcmc_step == \"gaussian_scalar\" and p != \"tdp full\": \n",
    "        #     continue \n",
    "        # else:\n",
    "        aps = [] \n",
    "        lrs = [] \n",
    "        sigmas = [0.5,1,1.5]\n",
    "        step_sizes = [0.01, 0.05, 0.1]\n",
    "        if p == \"ssgp\": \n",
    "            sigmas = [None] \n",
    "        for step_size in step_sizes: \n",
    "            for sigma in sigmas: \n",
    "                if p == \"ssgp\": \n",
    "                    prior = SkewSymmetricGaussianPrior()\n",
    "                elif p == \"tdp single\": \n",
    "                    prior = TimeDependentPrior(observation_times, R_max = 5, sigma = sigma, full = False) \n",
    "                elif p == \"tdp full\": \n",
    "                    prior = TimeDependentPrior(observation_times, R_max = 5, sigma = sigma, full = True)\n",
    "\n",
    "                pw = PolicyWalk(env,prior,observations,observation_times,alpha,mcmc_step = mcmc_step)\n",
    "                learned_rewards, acceptance_probs = pw.get_samples(step_size, n_iters = 100000)\n",
    "                aps.append(acceptance_probs) \n",
    "                lrs.append(learned_rewards)\n",
    "        \n",
    "        name = p + \" \" + mcmc_step\n",
    "        folderloc, AP_folder_loc, LR_folder_loc, chain_folder_loc = save(False, name)\n",
    "        doAcceptanceAnalysis(AP_folder_loc)\n",
    "        doChainAnalysis(chain_folder_loc)\n",
    "        doLearnedRewardAnalysis(0,2, LR_folder_loc)\n",
    "        doLearnedRewardAnalysis(1,2, LR_folder_loc)\n",
    "        doLossAnalysis(folderloc)\n",
    "\n",
    "# # policy_walk_iters = 1\n",
    "# # for _ in range(policy_walk_iters-1): \n",
    "# #     learned_rewards = np.concatenate((learned_rewards, pw.get_samples(step_size, n_iters = 150000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = p + \" \" + mcmc_step\n",
    "step_sizes = [0.005,0.01,0.05,0.1,0.5]\n",
    "folderloc, AP_folder_loc, LR_folder_loc, chain_folder_loc = save(True, name)\n",
    "doAcceptanceAnalysis(AP_folder_loc)\n",
    "doChainAnalysis(chain_folder_loc)\n",
    "doLearnedRewardAnalysis(0,2, LR_folder_loc)\n",
    "doLossAnalysis(folderloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folders(single, title): \n",
    "    if single: \n",
    "        folderloc = \"Single State Experiments/\" + title +\"/\"\n",
    "    else:\n",
    "        folderloc = \"Double State Experiments/\" + title + \"/\"\n",
    "    AP_folder_loc = folderloc + \"/Screenshots/Acceptance Probabilities/\"\n",
    "    LR_folder_loc = folderloc + \"/Screenshots/Rewards/\"\n",
    "    chain_folder_loc = folderloc + \"/Screenshots/Chain/\"\n",
    "    return folderloc, AP_folder_loc, LR_folder_loc, chain_folder_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder handling for saving experiment results \n",
    "\n",
    "def save(single, title):\n",
    "    if single: \n",
    "        folderloc = \"Single State Experiments/\" + title +\"/\"\n",
    "    else:\n",
    "        folderloc = \"Double State Experiments/\" + title + \"/\"\n",
    "    AP_folder_loc = folderloc + \"/Screenshots/Acceptance Probabilities/\"\n",
    "    LR_folder_loc = folderloc + \"/Screenshots/Rewards/\"\n",
    "    chain_folder_loc = folderloc + \"/Screenshots/Chain/\"\n",
    "    if not os.path.isdir(folderloc): \n",
    "        os.mkdir(folderloc) \n",
    "    if not os.path.isdir(folderloc + \"/Screenshots/\"):    \n",
    "        os.mkdir(folderloc + \"/Screenshots/\")\n",
    "    if not os.path.isdir(AP_folder_loc): \n",
    "        os.mkdir(AP_folder_loc)\n",
    "    if not os.path.isdir(LR_folder_loc): \n",
    "        os.mkdir(LR_folder_loc)\n",
    "    if not os.path.isdir(chain_folder_loc): \n",
    "        os.mkdir(chain_folder_loc)\n",
    "    np.save(folderloc + \"aps\", aps) \n",
    "    np.save(folderloc + \"lrs\", lrs) \n",
    "    f = open(folderloc + \"info.txt\", \"w+\") \n",
    "    f.write(\"Sigmas: \" + format(sigmas) +\"\\r\\n\")\n",
    "    f.write(\"Step Sizes: \" + format(step_sizes) + \"\\r\\n\") \n",
    "    f.write(\"Prior: \" + format(prior)+ \"\\r\\n\") \n",
    "    f.write(\"Jumping Distribution: \" + mcmc_step + \"\\r\\n\")\n",
    "    f.write(\"Alpha: \" + format(alpha)) \n",
    "    f.close()\n",
    "    return folderloc, AP_folder_loc, LR_folder_loc, chain_folder_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\":    \"16.0\",\n",
    "    \"figure.figsize\" : \"8, 5\",\n",
    "    \"axes.titlesize\": \"medium\",\n",
    "    \"figure.titlesize\": \"medium\",\n",
    "    \"savefig.dpi\" : \"250\",\n",
    "    \"savefig.format\" : \"png\",\n",
    "    \"savefig.bbox\": \"tight\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Acceptance Probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of thinned acceptance probabilities - change policy_walk to return acceptance probabilities to use\n",
    "# print(np.average(acceptance_probs))\n",
    "# plt.scatter(range(len(acceptance_probs[14::15])), acceptance_probs[14::15])\n",
    "# plt.show()\n",
    "import math\n",
    "# Moving average of acceptance probabilities\n",
    "def doAcceptanceAnalysis(folder):\n",
    "    i = 0 \n",
    "    for step_size in step_sizes: \n",
    "        for sigma in sigmas: \n",
    "            acceptance_probs = aps[i]\n",
    "            window_size = 500\n",
    "            n_windows = math.floor(len(acceptance_probs)/window_size) \n",
    "            averages = [] \n",
    "            for j in range(n_windows): \n",
    "                averages.append(np.average(acceptance_probs[window_size*i:window_size*(j+1)]))\n",
    "            plt.plot(np.arange(0, n_windows * window_size, window_size), averages, label = \"Step: \" + format(step_size))\n",
    "            plt.xlabel(\"Sample Number\")\n",
    "            plt.ylabel(\"Avg Acceptance Probability\")\n",
    "            if sigma is None: \n",
    "                plt.title(\"Acceptance Probabilities (Step: \" + format(step_size) +\")\")\n",
    "            elif step_size is None: \n",
    "                plt.title(\"Acceptance Probabilities (sigma: \" + format(sigma)+ \")\")\n",
    "            else: \n",
    "                plt.title(\"Acceptance Probabilities (Step: \" + format(step_size) +\", sigma: \" + format(sigma)+ \")\")\n",
    "            name = \"step \" + format(step_size) + \" sigma \" + format(sigma) + \".png\"\n",
    "            i+=1 \n",
    "    # name = \"all step sizes.png\"\n",
    "            if sigma is not None: \n",
    "\n",
    "                plt.savefig(folder + name, format = \"png\")\n",
    "                plt.show() \n",
    "    if sigma is None: \n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.savefig(folder + \"all step sizes.png\", format = \"png\") \n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Average Learned Reward Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the overall learned rewards - switch to the bigger colors array to view all \n",
    "colors = [\"#e71d43\",\"#ff0000\",\"#ff3700\",\"#ff6e00\",\"#ffa500\",\"#ffc300\",\"#ffe100\",\"#ffff00\",\"#aad500\",\"#55aa00\",\"#008000\",\"#005555\",\"#002baa\",\"#0000ff\",\"#1900d5\",\"#3200ac\",\"#4b0082\",\"#812ba6\",\"#b857ca\",\"#d03a87\"]\n",
    "# colors = [\"red\", \"orange\",\"yellow\",\"green\",\"blue\",\"indigo\",\"violet\"]\n",
    "\n",
    "# s,a,T = 0,0,10 \n",
    "T = 10\n",
    "def doLearnedRewardAnalysis(s,a, folder):\n",
    "    i = 0 \n",
    "    for step_size in step_sizes: \n",
    "        for sigma in sigmas: \n",
    "            lr = lrs[i]\n",
    "            avg_reward = np.average(lr[50000:], axis = 0)\n",
    "            for t in range(T):\n",
    "                label = \"t = \" + format(round(observation_times[t],2))\n",
    "                plt.plot(avg_reward[s,:,t], color = colors[t*2], label = label)\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.xlabel(\"Action\")\n",
    "            \n",
    "            if sigma is None: \n",
    "                plt.title(\"Learned Reward (Step: \" + format(step_size) +\")\")\n",
    "            elif step_size is None: \n",
    "                plt.title(\"Learned Reward (sigma: \" + format(sigma)+ \")\")\n",
    "            else: \n",
    "                plt.title(\"Learned Reward (Step: \" + format(step_size) +\", sigma: \" + format(sigma)+ \")\")\n",
    "            plt.legend(loc = \"upper right\")\n",
    "            if s == 0: \n",
    "                name = \"step \" + format(step_size) + \" sigma \" + format(sigma) + \".png\"\n",
    "            else:\n",
    "                name = \"Hungry step \" + format(step_size) + \" sigma \" + format(sigma) + \".png\"\n",
    "            plt.savefig(folder + name, format = \"png\") \n",
    "            plt.show()\n",
    "            i+=1\n",
    "\n",
    "    for t in range(T):\n",
    "        label = \"t = \" + format(round(observation_times[t],2))\n",
    "        plt.plot(R[s,:,t], color=colors[t*2], label = label)\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.xlabel(\"Actions\")\n",
    "    plt.title(\"True Reward\") \n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.savefig(folder + \"True Reward.png\", format = \"png\") \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Plotting samples for different sigma values. \n",
    "# for sigma in [0.001, 0.01, 0.1, 1, 2, 5, 100, 1000]:\n",
    "    \n",
    "    # prior = TimeDependentPrior(observation_times, R_max = 5, sigma = sigma)\n",
    "    # prior_sample = prior.sample(env.n_states, env.n_actions, trajectory_length)\n",
    "    # for t in range(T):\n",
    "    #     label = \"t = \" + format(round(observation_times[t],2))\n",
    "    #     plt.plot(prior_sample[s,:,t], color=colors[t*2], label = label)\n",
    "    # # for a in [0,4]:\n",
    "    # #     plt.plot(observation_times, prior_sample[s,a,:], label = \"a = \" + format(a))\n",
    "    # plt.ylabel(\"Reward\")\n",
    "    # # plt.xlabel(\"Time\")\n",
    "    # plt.xlabel(\"Actions\")\n",
    "    # plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def l2loss(reward, true_reward): \n",
    "    return np.linalg.norm((reward - true_reward).flatten(), 2) \n",
    "\n",
    "def doLossAnalysis(folderloc):\n",
    "    losses = []\n",
    "    xaxis = []\n",
    "\n",
    "    prev_sigma = None  # Variable to track the previous sigma value\n",
    "    i = 0\n",
    "    f = open(folderloc + \"losses.txt\", \"w+\") \n",
    "    for step_size in step_sizes:\n",
    "        for sigma in sigmas: \n",
    "            lr = lrs[i] \n",
    "            loss = l2loss(np.average(lr[50000:], axis = 0), R)\n",
    "            f.write(format(loss) + \"\\n\")\n",
    "            losses.append(loss)\n",
    "            xaxis.append((step_size, sigma))\n",
    "            i += 1\n",
    "    if sigma is not None: \n",
    "        plt.xlabel(\"Sigma\")\n",
    "    plt.ylabel(\"L2 Loss\")\n",
    "    plt.title(\"L2 Losses of Learned Rewards for Different Hyperparameters\")\n",
    "\n",
    "    # Set up step size colors and legend\n",
    "    step_size_colors = plt.cm.get_cmap(\"rainbow\", len(step_sizes))\n",
    "    step_size_labels = [f\"Step Size: {step_size}\" for step_size in step_sizes]\n",
    "    step_size_patches = [plt.Rectangle((0, 0), 1, 1, color=step_size_colors(i)) for i in range(len(step_sizes))]\n",
    "    plt.legend(step_size_patches, step_size_labels, loc=\"lower right\")\n",
    "\n",
    "    # Create the bar chart\n",
    "    bar_width = 3 / len(sigmas)  # Width of each bar\n",
    "    x_positions = np.arange(len(xaxis))\n",
    "    xtick_positions = []  # Positions for x-axis tick labels\n",
    "    xtick_labels = []  # Labels for x-axis tick labels\n",
    "\n",
    "    for i, (step_size, sigma) in enumerate(xaxis):\n",
    "        if sigma != prev_sigma:  # Check if sigma has changed\n",
    "            xtick_positions.append(x_positions[i])\n",
    "            xtick_labels.append(sigma)\n",
    "            prev_sigma = sigma\n",
    "\n",
    "        plt.bar(x_positions[i], losses[i], width=bar_width, color=step_size_colors(step_sizes.index(step_size)))\n",
    "\n",
    "    plt.plot([np.min(losses)]*len(sigmas)*len(step_sizes), color = 'black', linestyle = ':', label = 'Min: ' + format(np.min(losses)))\n",
    "\n",
    "    # Adjust x-axis ticks and labels\n",
    "    plt.xticks(xtick_positions, xtick_labels, rotation='vertical')\n",
    "    name = \"L2 Losses of Learned Rewards for Different Hyperparameters.png\"\n",
    "    plt.savefig(folderloc + \"/Screenshots/\" + name, format = \"png\", ) \n",
    "    plt.show()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spotify Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#e71d43\",\"#ff0000\",\"#ff3700\",\"#ff6e00\",\"#ffa500\",\"#ffc300\",\"#ffe100\",\"#ffff00\",\"#aad500\",\"#55aa00\",\"#008000\",\"#005555\",\"#002baa\",\"#0000ff\",\"#1900d5\",\"#3200ac\",\"#4b0082\",\"#812ba6\",\"#b857ca\",\"#d03a87\"]\n",
    "avg_reward = np.average(np.asarray(learned_rewards[200000:]),axis = 0)\n",
    "# print(observation_times)\n",
    "actions = ['Sea Girls', 'Christmas', 'Mature Cheddar', 'Renault Megane']\n",
    "import pandas as pd \n",
    "date_range = pd.date_range(start='2021-12-09', periods = len(observation_times), freq = 'D')\n",
    "# date_range = pd.date_range(start='2021-12-09', end='2023-04-28', freq='D')\n",
    "highlight = 3\n",
    "for a in range(0,4):\n",
    "    alpha = 0.25 \n",
    "    if a == highlight:\n",
    "        alpha = 1 \n",
    "    plt.plot(date_range, avg_reward[s,a,:], label = actions[a], alpha = alpha)\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to analyse individual chains: \n",
    "#Plot a single reward\n",
    "s,a,t = 0,0,1\n",
    "plt.plot(lrs[2][:,s,a,t])\n",
    "ylabel = \"Reward of (s,a,t) = (\" + format(s) + \",\" + format(a) + \",\" + format(t) + \")\"\n",
    "plt.ylabel(ylabel)\n",
    "plt.xlabel(\"Action\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective Sample Size Calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = tfp.mcmc.effective_sample_size(lrs[4][20000:])\n",
    "print(np.average(ess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot rewards over time for a given state/action pair - shows observed (s,a,t) pairs in colour \n",
    "# print(observations)\n",
    "\n",
    "#I could try plotting like true reward vs assigned reward over time or something to better visualise idk oh maybe do colours based on reward \n",
    "def doChainAnalysis(folder):\n",
    "    i = 0 \n",
    "    colors = [\"#e71d43\",\"#ff0000\",\"#ff3700\",\"#ff6e00\",\"#ffa500\",\"#ffc300\",\"#ffe100\",\"#ffff00\",\"#aad500\",\"#55aa00\",\"#008000\",\"#005555\",\"#002baa\",\"#0000ff\",\"#1900d5\",\"#3200ac\",\"#4b0082\",\"#812ba6\",\"#b857ca\",\"#d03a87\"]\n",
    "    for step_size in step_sizes: \n",
    "        for sigma in sigmas: \n",
    "            lr = lrs[i]\n",
    "            s,a, = 0,2\n",
    "            # s,t = 0,5\n",
    "            for t in range(trajectory_length):\n",
    "                label = \"t = \" + format(round(observation_times[t],1)) + \" (\" + format(round(R[s,a,t],1)) + \")\"\n",
    "                if R[s,a,t] > 0.25:\n",
    "                    plt.plot(lr[:,s,a,t], color=colors[t*2], label = label)\n",
    "                else:\n",
    "                    plt.plot(lr[:,s,a,t], color=\"grey\", label = label)        \n",
    "                \n",
    "            plt.ylabel(\"R(\" + format(s) + \",\" + format(a) + \",t)\")\n",
    "            plt.xlabel(\"Iteration\")\n",
    "            #don't need to do the sigma here, just copy the none code over from the other bit and do the same thing here.\n",
    "            if sigma is None: \n",
    "                plt.title(\"Reward throughout Sampling (Step: \" + format(step_size) +\")\")\n",
    "            elif step_size is None: \n",
    "                plt.title(\"Reward throughout Sampling (sigma: \" + format(sigma)+ \")\")\n",
    "            else: \n",
    "                plt.title(\"Reward throughout Sampling (Step: \" + format(step_size) +\", sigma: \" + format(sigma)+ \")\")\n",
    "            plt.legend()\n",
    "            name = \"step \" + format(step_size) + \" sigma \" + format(sigma) + \".png\"\n",
    "            plt.savefig(folder + name, format = \"png\") \n",
    "            plt.show()\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_rewards = learned_rewards[37500:]\n",
    "np.swapaxes(converged_rewards,1,2)\n",
    "converged_rewards = np.reshape(converged_rewards, (converged_rewards.shape[0], converged_rewards.shape[1]*converged_rewards.shape[2], converged_rewards.shape[3]))\n",
    "print(converged_rewards.shape)\n",
    "print(observation_times)\n",
    "s,a,t = 0,2,0\n",
    "for t in range(20):\n",
    "    plt.hist(converged_rewards[:,s,t],bins = 20,color=colors[t])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Double State Experiments/tdp full grid_walk/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folderloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned_rewards.shape\n",
    "# #Flatten the rewards \n",
    "best_lrs = np.load(\"Double State Experiments/tdp single gaussian_scalar/lrs.npy\")\n",
    "best_lr = best_lrs[4] \n",
    "# best_learned = np.average(best_lr[50000:], axis = 0) \n",
    "# print(l2loss(best_learned,R))\n",
    "\n",
    "converged_rewards = best_lr[50000:]\n",
    "flattened_rewards = converged_rewards.reshape(converged_rewards.shape[0], -1)\n",
    "cov = np.cov(flattened_rewards.T)\n",
    "# from statsmodels.stats.correlation_tools import cov_nearest\n",
    "# pd_cov = cov_nearest(cov, threshold = 1e-8)\n",
    "\n",
    "np.save(\"Double State Experiments/best_cov.npy\", cov)\n",
    "\n",
    "# zero_mean = np.zeros(flattened_rewards.shape[1])\n",
    "# test = np.random.normal(np.zeros(flattened_rewards.shape[1]))\n",
    "# print(pd_cov)\n",
    "# np.save(folderloc,cov)\n",
    "# print(cov)\n",
    "# np.save('learned_rewards_short_spotify',learned_rewards)\n",
    "# np.save('acceptance_probs_short_spotify', acceptance_probs)\n",
    "# learned_rewards = np.load('learned_rewards_full_spotify.npy')\n",
    "# acceptance_probs = np.load('acceptance_probs_full_spotify.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ReDoing Graphs \n",
    "trajectory_length = 10\n",
    "env = DoubleStateSpace(n_actions = 10, discount_rate = 0.1, R_max = 5)\n",
    "\n",
    "# #generate observation times \n",
    "observation_times = np.load(\"Double State Experiments/observation_times.npy\")\n",
    "\n",
    "# # Create a matrix for the true rewards \n",
    "s_grid, a_grid, t_grid = np.meshgrid(env.states, env.actions, observation_times, indexing='ij')\n",
    "R = np.vectorize(env.reward)(s_grid,a_grid,t_grid)\n",
    "\n",
    "observations = np.load(\"Double State Experiments/observations.npy\")\n",
    "\n",
    "mcmc_steps = [\"gaussian_scalar\",\"grid_walk\"]\n",
    "priors = [\"ssgp\", \"tdp single\", \"tdp full\"]\n",
    "for mcmc_step in mcmc_steps: \n",
    "    for p in priors: \n",
    "        aps = [] \n",
    "        lrs = [] \n",
    "        sigmas = [0.5,1,1.5]\n",
    "        step_sizes = [0.01, 0.05, 0.1]\n",
    "        if p == \"ssgp\": \n",
    "            sigmas = [None] \n",
    "        \n",
    "        name = p + \" \" + mcmc_step\n",
    "        folderloc, AP_folder_loc, LR_folder_loc, chain_folder_loc = get_folders(False, name)\n",
    "        lrs = np.load(folderloc + \"lrs.npy\") \n",
    "        aps = np.load(folderloc + \"aps.npy\")\n",
    "        doAcceptanceAnalysis(AP_folder_loc)\n",
    "        doChainAnalysis(chain_folder_loc)\n",
    "        doLearnedRewardAnalysis(0,2, LR_folder_loc)\n",
    "        doLearnedRewardAnalysis(1,2, LR_folder_loc)\n",
    "        doLossAnalysis(folderloc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb6416ccb7cf3862052581a5888563209785f6bca63738835247a0de3829cc25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
